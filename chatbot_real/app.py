import os
import re
import streamlit as st

# â”€â”€ ëª½í‚¤íŒ¨ì¹˜: langchain_community êµ¬ë²„ì „ ChatCompletion.create â†’ v1 API ë§¤í•‘ â”€â”€
import openai as _openai_module
from openai import OpenAI as OpenAIClient

def _legacy_chat_completion_create(
    *, model=None, model_name=None, openai_api_key=None, **kwargs
):
    actual_model = model or model_name
    key = openai_api_key or st.secrets["OPENAI_API_KEY"]
    client = OpenAIClient(api_key=key)
    return client.chat.completions.create(model=actual_model, **kwargs)

_openai_module.ChatCompletion = type(
    "ChatCompletion", (), {"create": staticmethod(_legacy_chat_completion_create)}
)

# â”€â”€ ë‚˜ë¨¸ì§€ import â”€â”€
from openai import OpenAI as OpenAIClientRaw
from langchain_community.document_loaders import PDFPlumberLoader
from langchain_community.embeddings.openai import OpenAIEmbeddings
from langchain_community.vectorstores.chroma import Chroma
from langchain_community.llms.openai import OpenAI as CC_OpenAI
from langchain.chains import RetrievalQA
from langchain.text_splitter import RecursiveCharacterTextSplitter
import fitz

# Streamlit ë ˆì´ì•„ì›ƒ
st.set_page_config(layout="wide", page_title="ì „ìì •ë¶€ í”„ë ˆì„ì›Œí¬ ì±—ë´‡")
col1, col2 = st.columns([2, 1])

# â–¶ ì˜¤ë¥¸ìª½: PDF ì—…ë¡œë“œ & ì¸ë±ìŠ¤ 1íšŒ ìƒì„±
with col2:
    st.header("PDF ì—…ë¡œë“œ ë° ë·°ì–´")
    uploaded = st.file_uploader("PDF ì—…ë¡œë“œ", type="pdf")
    if uploaded and "vectordb" not in st.session_state:
        st.info("PDF ì¸ë±ì‹± ì¤‘â€¦")
        path = "uploaded_egov.pdf"
        with open(path, "wb") as f:
            f.write(uploaded.getbuffer())
        st.session_state["uploaded_path"] = path

        loader = PDFPlumberLoader(path)
        docs = loader.load()
        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        chunks = splitter.split_documents(docs)

        embeddings = OpenAIEmbeddings()
        vectordb = Chroma.from_documents(chunks, embeddings, persist_directory="egov_chroma_db")
        st.session_state.vectordb = vectordb

        pdf_doc = fitz.open(stream=uploaded.getbuffer(), filetype="pdf")
        st.session_state.page_images = [
            page.get_pixmap(dpi=150).tobytes("png") for page in pdf_doc
        ]
        st.success("ì—…ë¡œë“œ ì™„ë£Œ! ğŸ‰")

    if "page_images" in st.session_state:
        st.subheader("PDF ë¯¸ë¦¬ë³´ê¸°")
        for i, img in enumerate(st.session_state.page_images[:5], 1):
            st.image(img, caption=f"Page {i}", width=100)

# â–¶ ì™¼ìª½: ì±—ë´‡ UI
with col1:
    st.title("ì „ìì •ë¶€ í”„ë ˆì„ì›Œí¬ ì±—ë´‡")
    if "openai_model" not in st.session_state:
        st.session_state["openai_model"] = "gpt-3.5-turbo"

    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì´ˆê¸°í™”
    if "initialized" not in st.session_state:
        st.session_state.messages = [{
            "role": "system",
            "content": (
                "ì „ìì •ë¶€ í‘œì¤€í”„ë ˆì„ì›Œí¬ ê´€ë ¨ ì§ˆë¬¸ì€ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ê³ , "
                "ê·¸ ì™¸ ì¼ë°˜ ëŒ€í™”(ì¸ì‚¬, ì¡ë‹´ ë“±)ëŠ” ìì—°ìŠ¤ëŸ½ê²Œ í•œêµ­ì–´ë¡œ ì‘ë‹µí•˜ì„¸ìš”."
            )
        }]
        st.session_state.initialized = True

    # í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„
    raw_client = OpenAIClientRaw(api_key=st.secrets["OPENAI_API_KEY"])
    chain_llm  = CC_OpenAI(
        model_name=st.session_state.get("openai_model", "gpt-3.5-turbo"),
        openai_api_key=st.secrets["OPENAI_API_KEY"],
        temperature=0
    )

    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        reply = None
        docs_and_scores = []

        # 1) ì¸ì‚¬ ì²˜ë¦¬
        if prompt.strip().lower() in {"ì•ˆë…•","ì•ˆë…•í•˜ì„¸ìš”","hi","hello"}:
            resp = raw_client.chat.completions.create(
                model=st.session_state["openai_model"],
                messages=st.session_state.messages
            )
            reply = resp.choices[0].message.content

        # 2) í˜ì´ì§€ ìš”ì•½ ì²˜ë¦¬
        else:
            m = re.match(r"(\d+)\s*í˜ì´ì§€ ìš”ì•½", prompt)
            if m and "uploaded_path" in st.session_state:
                target = int(m.group(1)) - 1
                loader = PDFPlumberLoader(st.session_state["uploaded_path"])
                all_docs = loader.load()
                page_texts = [
                    d.page_content
                    for d in all_docs
                    if d.metadata.get("page",1)-1 == target
                ]
                to_sum = "\n".join(page_texts)
                resp = raw_client.chat.completions.create(
                    model=st.session_state["openai_model"],
                    messages=[
                        {"role":"system","content":"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ê°„ê²°íˆ ìš”ì•½í•´ ì£¼ì„¸ìš”."},
                        {"role":"user","content":to_sum}
                    ]
                )
                reply = resp.choices[0].message.content
                docs_and_scores = [{"metadata":{"page":target+1}}]

        # 3) ì¼ë°˜ RAG vs GPT
        if reply is None and "vectordb" in st.session_state:
            retriever = st.session_state.vectordb.as_retriever(search_kwargs={"k":3})
            docs_and_scores = retriever.get_relevant_documents(prompt)
            if docs_and_scores:
                qa = RetrievalQA.from_chain_type(
                    llm=chain_llm, chain_type="stuff", retriever=retriever
                )
                reply = qa.run(prompt)

                # RAGì´ â€œëª¨ë¥´ê² ë‹¤â€ë¥˜ë©´ GPTë¡œ ì¬ì‹œë„
                if any(kw in reply.lower() for kw in ["i'm sorry","don't know","ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"]):
                    reply = None
                    docs_and_scores = []

        # 4) ìµœì¢… GPT fallback
        if reply is None:
            resp = raw_client.chat.completions.create(
                model=st.session_state["openai_model"],
                messages=st.session_state.messages
            )
            reply = resp.choices[0].message.content

        st.session_state.messages.append({"role":"assistant","content":reply})
        with st.chat_message("assistant"):
            st.markdown(reply)

        # 5) RAG ëª¨ë“œì¼ ë•Œë§Œ í˜ì´ì§€ ì´ë¯¸ì§€
        if docs_and_scores:
            pages = sorted({
                (d["metadata"]["page"]-1) if isinstance(d, dict) else d.metadata.get("page",1)-1
                for d in docs_and_scores
            })
            for pg in pages:
                with st.expander(f"ê´€ë ¨ í˜ì´ì§€: {pg+1}"):
                    st.image(st.session_state.page_images[pg], use_container_width=True)
                    
                    















