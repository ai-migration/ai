import requests
import json
from bs4 import BeautifulSoup
import urllib.parse
from openai import OpenAI
import faiss
import numpy as np
import pandas as pd
from tqdm import tqdm
import tiktoken
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.docstore.document import Document
import os
import re

OWNER = "eGovFramework"
REPO = "egovframe-common-components"
BRANCH = "main"
TARGET_DIR = "src/main/java"
BASE_URL = "https://www.egovframe.go.kr"
INIT_GUIDE_URL = f"{BASE_URL}/wiki/doku.php?id=egovframework:com:v4.3:init_guide"
# ìµœëŒ€ í† í° ìˆ˜
MAX_TOKENS = 8192
EMBED_MODEL = "text-embedding-3-small"

# ë„ë©”ì¸(ìƒìœ„í´ë”), ê¸°ëŠ¥(ì¤‘ê°„), ê³„ì¸µ(Controller/DAO ë“±) ë¶„ë¦¬
def classify_path_info(path):
    '''
    src/main/java/egovframework/com/sym/mnu/mcm/service/MenuCreatVO.java -> service ê²½ë¡œì— ìˆì§€ë§Œ VOë¡œ ë¶„ë¦¬
    src/main/java/egovframework/com/cop/bbs/web/EgovBBSController.java
    src/main/java/egovframework/com/sym/log/wlg/web/EgovWebLogInterceptor.java -> controllerê°€ ì—†ì§€ë§Œ webì— ìˆìœ¼ë‹ˆê¹Œ controllerë¡œ ë¶„ë¦¬
    src/main/java/egovframework/com/sym/log/wlg/service/impl/EgovWebLogServiceImpl.java
    '''
    segments = path.split('/')
    domain = segments[5] if len(segments) > 5 else "unknown"  # cop, sym, uss ë“±
    feature = segments[6] if len(segments) > 6 and not segments[6].endswith(".java") else "common"
    filename = os.path.basename(path)

    dir, filename = os.path.split(path)

    if 'controller' in filename.lower() or 'web' in dir.lower():
        component = 'Controller'
    elif 'serviceimple' in filename.lower() or 'impl' in dir.lower():
        component = 'ServiceImpl'
    elif 'vo' in filename.lower():
        component = 'VO'
    elif 'mapper' in dir.lower():
        component = 'Mapper'
    elif 'handler' in dir.lower() or 'hndlr' in dir.lower() or 'handler' in filename.lower():
        component = 'Handler'
    elif 'service' in filename.lower() or ('service' in dir.lower() and ('vo' not in filename.lower() or 'serviceimple' in filename.lower())):
        component = 'Service'
    else:
        component = 'Other'
    return domain, feature, component

# GitHub APIë¡œ ì „ì²´ íŠ¸ë¦¬ ê°€ì ¸ì˜¤ê¸°
def fetch_file_list():
    url = f"https://api.github.com/repos/{OWNER}/{REPO}/git/trees/{BRANCH}?recursive=1"
    res = requests.get(url)
    res.raise_for_status()
    data = res.json()
    files = data["tree"]
    java_files = [f for f in files if f["path"].startswith(TARGET_DIR) and f["path"].endswith(".java")]
    return java_files

# ì½”ë“œ ìˆ˜ì§‘ ë° ì €ì¥
def collect_egov_code(descriptions, save_path):
    java_files = fetch_file_list()
    raw_base = f"https://raw.githubusercontent.com/{OWNER}/{REPO}/{BRANCH}"

    with open(save_path, "w", encoding="utf-8") as out_f:
        for f in java_files:
            rel_path = f["path"]

            filename = rel_path.split("/")[-1]
            raw_url = f"{raw_base}/{rel_path}"
            domain, feature, component = classify_path_info(rel_path)

            if component not in ('Controller', 'Service', 'ServiceImpl', 'DAO', 'VO'):
              continue
 
            try:
                code = requests.get(raw_url).text.strip()

                entry = {
                    "description": descriptions.get(filename, ''),
                    "title": filename.replace(".java", ""),
                    "path": rel_path,
                    "domain": domain,      # ì˜ˆ: cop
                    "feature": feature,    # ì˜ˆ: bbs
                    "type": component,     # ì˜ˆ: Controllers
                    "code": code
                }
                out_f.write(json.dumps(entry, ensure_ascii=False) + "\n")
            except Exception as e:
                print(f"âš ï¸ Error loading {raw_url}: {e}")

# ê³µí†µ ì»´í¬ë„ŒíŠ¸ ê°€ì´ë“œ í˜ì´ì§€ì—ì„œ ëª¨ë“  í˜ì´ì§€ ìˆ˜ì§‘
def get_subpage_links(url: str) -> list:
    res = requests.get(url)
    res.encoding = "utf-8"
    soup = BeautifulSoup(res.text, 'html.parser')

    links = []
    for ul in soup.find_all("ul"):
        for a in ul.find_all("a", href=True):
            href = a["href"]
            if href.startswith('/wiki/doku.php?id=egovframework:')  and 'init_guide' not in href:
            # if (href.startswith("/wiki/doku.php?id=egovframework:com:v4.3:") or href.startswith("/wiki/doku.php?id=egovframework:com:v3")) and 'init_guide' not in href:
                page_url = urllib.parse.urljoin(BASE_URL, href)
                page_name = a.get_text(strip=True)
                print(f"ğŸ” ë§í¬ ìˆ˜ì§‘ ì¤‘: {page_name} ({page_url})")
                links.append({"url": page_url, "title": page_name})
    return links

# ì½©í†µ ì»´í¬ë„ŒíŠ¸ ì½”ë“œ íŒŒì¼ì— ëŒ€í•œ ì„¤ëª…
def get_code_description(pages) -> dict:
    """
    ê° í•˜ìœ„ í˜ì´ì§€ì—ì„œ ìœ í˜•/ëŒ€ìƒì†ŒìŠ¤ëª…/ë¹„ê³  í‘œ ì¶”ì¶œ
    """
    descriptions = {}
    for page in pages:
        print(f"ğŸ” ì„¤ëª… ìˆ˜ì§‘ ì¤‘: {page['title']} ")
        response = requests.get(page['url'])
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        tables = soup.find_all("table")

        for table in tables:
            headers = [th.get_text(strip=True) for th in table.find_all("th")]
            if not ({"ìœ í˜•", "ë¹„ê³ ", "ëŒ€ìƒì†ŒìŠ¤ëª…"}.issubset(set(headers)) or 
                    {"ìœ í˜•", "ë¹„ê³ ", "ëŒ€ìƒì†ŒìŠ¤"}.issubset(set(headers)) or
                    {"ìœ í˜•", "ì†ŒìŠ¤", "ì„¤ëª…"}.issubset(set(headers)) or
                    {"ìœ í˜•", "ëŒ€ìƒì†ŒìŠ¤ëª…", "ì„¤ëª…", "ë¹„ê³ "}.issubset(set(headers))):
                continue

            for row in table.find_all("tr")[1:]:
                cells = row.find_all("td")

                if len(cells) < 3:
                    continue

                if cells[1].get_text(strip=True).endswith('.java'):
                    filename = re.split(r'[./]', cells[1].get_text(strip=True))[-2] + '.java'

                    # ì„¤ëª… ì¶”ì¶œ
                    if len(headers) == 4:
                        parts = [cells[2].get_text(strip=True), cells[3].get_text(strip=True)]
                    elif len(headers) == 3:
                        parts = [cells[2].get_text(strip=True)]
                    else:
                        parts = []

                    # ë¹ˆ ë¬¸ìì—´ ì œê±° í›„ ì¡°ì¸
                    desc = ', '.join([p for p in parts if p])

                    if filename in descriptions:
                        descriptions[filename] += f", {desc}"
                    else:
                        descriptions[filename] = desc

            break   
    
    # ì¤‘ë³µ ì„¤ëª… ì œê±°
    for k, v in descriptions.items():
        res = ', '.join(dict.fromkeys(item.strip() for item in v.split(',')))
        descriptions[k] = res
    return descriptions

def build_vectordb(jsonpath, dbpath, embedding_model):
    docs = []

    with open(jsonpath, 'r', encoding='utf-8') as f:
        for line in f:
            data = json.loads(line)
            # if not data['ì½”ë“œ']:
            #     continue

            doc = Document(
                page_content = f"[description] {data['description']}\n[role]{data['type']}\n[code]{data['code']}",
                metadata = {
                    "title": data["title"],
                    "path": data["path"],
                    "type": data["type"],
                    "domain": data["domain"],
                    "feature": data.get("feature", "unknown"),
                    "description": data["description"]
                }
            )
            docs.append(doc)
    
    first_doc = docs[0]
    vectorstore = FAISS.from_documents([first_doc], embedding_model)

    # ë‚˜ë¨¸ì§€ ë¬¸ì„œë“¤ í•˜ë‚˜ì”© ì¶”ê°€
    for doc in tqdm(docs[1:], desc="Embedding docs"):
        partial = FAISS.from_documents([doc], embedding_model)
        vectorstore.merge_from(partial)

    # ì €ì¥
    vectorstore.save_local(dbpath)

    # splitter = RecursiveCharacterTextSplitter(
    #     chunk_size=1000, chunk_overlap=100
    # )
    # split_docs = splitter.split_documents(docs)  # ì—¬ëŸ¬ ê°œë¡œ ë¶„í• ëœ Document ë¦¬ìŠ¤íŠ¸
    
    # vectorstore = FAISS.from_documents(split_docs, embedding_model)
    # vectorstore.save_local(dbpath)

    # vectorstore = FAISS.from_documents(docs, embedding_model)
    # vectorstore.save_local(dbpath)
    print(f"âœ… FAISS DB ì €ì¥ ì™„ë£Œ: {dbpath}/index.faiss")

if __name__ == "__main__":
    subpages = get_subpage_links(INIT_GUIDE_URL)
    descriptions = get_code_description(subpages)
    # with open("descriptions_0803.json", "w", encoding='utf-8') as json_file:
    #     json.dump(descriptions, json_file, ensure_ascii=False, indent=2)
    # collect_egov_code(descriptions, 'egovcode_0805.jsonl')

    embedding_model = OpenAIEmbeddings(model="text-embedding-3-small", api_key='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')
    build_vectordb('egovcode_0805.jsonl', 'eGovCodeDB_0805', embedding_model)
    
    # get_code_description([{'url': 'https://www.egovframe.go.kr/wiki/doku.php?id=egovframework:ldap%EC%A1%B0%EC%A7%81%EB%8F%84%EA%B4%80%EB%A6%AC', 
    #                        'title': 'LDAPì¡°ì§ë„ê´€ë¦¬(v3.2 ì‹ ê·œ)'}])
    # print(classify_path_info("src/main/java/egovframework/com/cmm/AltibaseClobStringTypeHandler.java"))