# rag_security_agent.py
import os
import json
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv
load_dotenv(dotenv_path=Path(__file__).with_name(".env"))

# === deps ===
from time import perf_counter
import numpy as np
from sentence_transformers import SentenceTransformer
from openai import OpenAI

import chromadb
from chromadb.config import Settings

# (ì„ íƒ) LLM ì—ì´ì „íŠ¸ë¡œ JSONì„ ë¬¸ì¥í™”/ë³´ì •í•˜ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©
from crewai import Agent, Task, Crew, LLM

# =========================
# ì„¤ì •
# =========================
BASE = Path(__file__).resolve().parent
AGENT_INPUTS_PATH = BASE / "outputs" / "agent_inputs.json"
OUTPUT_DIR = BASE / "outputs" / "security_reports"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
LIMIT_ISSUES = 5


# --- timeouts & skips ---
RAG_SKIP_TEXT = os.getenv("RAG_SKIP_TEXT", "0") == "1"
RAG_SKIP_CODE = os.getenv("RAG_SKIP_CODE", "0") == "1"
CHROMA_QUERY_TIMEOUT = float(os.getenv("CHROMA_QUERY_TIMEOUT", "15"))  # seconds


# Chroma í´ë”(í´ë” ì „ì²´ê°€ DB)
CHROMA_PERSIST_DIR = str(BASE)                         # í•„ìš” ì‹œ ./chroma ë“±ìœ¼ë¡œ ë³€ê²½
TEXT_COLLECTION_NAME = "security_text_e5l"            # ì¸ë±ì‹± ë•Œ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ì»¬ë ‰ì…˜ëª…
CODE_COLLECTION_NAME = "security_code_oai3l"          # ì¸ë±ì‹± ë•Œ ì‚¬ìš©í•œ ì½”ë“œ ì»¬ë ‰ì…˜ëª…

# OpenAI
OPENAI_CHAT_MODEL = os.getenv("OPENAI_CHAT_MODEL", "gpt-4o-mini")
OPENAI_EMBED_MODEL = "text-embedding-3-large"
OPENAI_EMBED_DIM: Optional[int] = None   # ì¸ë±ì‹± ë•Œ dimensionsë¥¼ ì¼ë‹¤ë©´ ë™ì¼ ìˆ˜ì¹˜(ì˜ˆ: 1024)

# =========================
# ìœ í‹¸
# =========================
def load_agent_inputs(path: Path) -> List[Dict[str, Any]]:
    data = json.loads(path.read_text(encoding="utf-8"))
    return data if isinstance(data, list) else [data]

EXT2LANG = {
    ".py":"python",".java":"java",".js":"javascript",".ts":"typescript",
    ".cs":"csharp",".cpp":"cpp",".c":"c",".go":"go",".rb":"ruby",".php":"php",
    ".kt":"kotlin",".rs":"rust"
}
def detect_lang(issue: Dict[str, Any]) -> Optional[str]:
    # 1) rule í”„ë¦¬í”½ìŠ¤ì—ì„œ ì¶”ì • (ì˜ˆ: python:S1481)
    rule = (issue.get("rule") or "")
    if ":" in rule:
        lang = rule.split(":",1)[0].strip().lower()
        if lang: return lang
    # 2) íŒŒì¼ í™•ì¥ìì—ì„œ ì¶”ì •
    comp = issue.get("component") or ""
    return EXT2LANG.get(Path(comp).suffix.lower())

def sanitize_filename(name: str) -> str:
    return name.replace("/", "_").replace("\\", "_").strip()

# =========================
# ì„ë² ë” (ì¿¼ë¦¬ ì „ìš©)
# =========================
_e5_query = SentenceTransformer("intfloat/multilingual-e5-large")

def embed_e5_query(q: str) -> List[float]:
    v = _e5_query.encode([f"query: {q}"], normalize_embeddings=True)[0]
    return v.tolist()

def embed_openai_query(q: str) -> List[float]:
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    resp = client.embeddings.create(model=OPENAI_EMBED_MODEL, input=[q], dimensions=OPENAI_EMBED_DIM)
    vec = np.array(resp.data[0].embedding, dtype=np.float32)
    vec = vec / (np.linalg.norm(vec) + 1e-12)  # L2
    return vec.tolist()

# =========================
# Chroma ì—°ê²°
# =========================
def init_chroma(persist_dir: str):
    return chromadb.PersistentClient(path=persist_dir, settings=Settings(anonymized_telemetry=False))

def get_text_code_collections(client):
    # ì´ë¦„ìœ¼ë¡œ ê°€ì ¸ì˜¤ë˜, ì‹¤íŒ¨ ì‹œ ëª©ë¡ ë…¸ì¶œ
    try:
        print("  â†’ get text collection ...")
        text_col = client.get_collection(TEXT_COLLECTION_NAME)
        print("    âœ“ text collection ok")
    except Exception as e:
        names = [c.name for c in client.list_collections()]
        raise RuntimeError(f"í…ìŠ¤íŠ¸ ì»¬ë ‰ì…˜ '{TEXT_COLLECTION_NAME}' ì—†ìŒ. ì‹¤ì œ ëª©ë¡: {names}") from e
    try:
        print("  â†’ get code collection ...")
        code_col = client.get_collection(CODE_COLLECTION_NAME)
        print("    âœ“ code collection ok")
    except Exception as e:
        names = [c.name for c in client.list_collections()]
        raise RuntimeError(f"ì½”ë“œ ì»¬ë ‰ì…˜ '{CODE_COLLECTION_NAME}' ì—†ìŒ. ì‹¤ì œ ëª©ë¡: {names}") from e
    return text_col, code_col

# =========================
# ê²€ìƒ‰ & ìœµí•© (RRF)
# =========================
def rrf(ids_a: List[str], ids_b: List[str], k=60, w_a=0.6, w_b=0.4) -> List[str]:
    score={}
    for r,i in enumerate(ids_a,1): score[i]=score.get(i,0)+w_a*(1/(k+r))
    for r,i in enumerate(ids_b,1): score[i]=score.get(i,0)+w_b*(1/(k+r))
    return sorted(score, key=score.get, reverse=True)

def query_collection(col, qvec, topk=5, where=None):
    return col.query(
        query_embeddings=[qvec],
        n_results=topk,
        include=["documents","metadatas","distances"],
        where=where
    )

from concurrent.futures import ThreadPoolExecutor, TimeoutError as FuturesTimeout
def query_collection_safe(col, qvec, topk=5, where=None, label="TEXT"):
    """
    Windows ì•ˆì „ ë²„ì „: ìŠ¤ë ˆë“œ/íƒ€ì„ì•„ì›ƒ ë¯¸ì‚¬ìš©.
    - RAG_FORCE_PEEK=1 ì´ë©´ query ìƒëµí•˜ê³  peek()ë§Œ ì‚¬ìš©
    - queryê°€ Python ë ˆë²¨ì—ì„œ ì˜ˆì™¸ë¥¼ ë˜ì§€ë©´ peek()ë¡œ í´ë°±
    """
    try:
        if os.getenv("RAG_FORCE_PEEK", "0") == "1":
            print(f"   {label} forced peek (RAG_FORCE_PEEK=1)")
            pk = col.peek()
            docs = (pk.get("documents") or [])
            docs = docs[:topk] if isinstance(docs, list) else []
            return {
                "ids": [[f"__peek__{i}" for i in range(len(docs))]],
                "documents": [docs],
                "metadatas": [[]],
                "distances": [[]],
            }

        # ë™ê¸° í˜¸ì¶œ(ìŠ¤ë ˆë“œX). include ìµœì†Œí™”ë¡œ ì§€ì—° ì¤„ì´ê¸°
        return col.query(
            query_embeddings=[qvec],
            n_results=topk,
            include=["documents"],     # ìµœì†Œ
            where=where
        )

    except Exception as e:
        print(f"   {label} query raised: {e} â†’ fallback to peek()")
        try:
            pk = col.peek()
            docs = (pk.get("documents") or [])
            docs = docs[:topk] if isinstance(docs, list) else []
            return {
                "ids": [[f"__peek__{i}" for i in range(len(docs))]],
                "documents": [docs],
                "metadatas": [[]],
                "distances": [[]],
            }
        except Exception as e2:
            print(f"   {label} peek failed: {e2}")
            return {"ids":[[]], "documents":[[]], "metadatas":[[]], "distances":[[]]}

def retrieve_context_dual(text_col, code_col, query: str, lang: Optional[str], k: int = 5) -> List[str]:
    # 1) TEXT(E5)
    if RAG_SKIP_TEXT:
        print("   TEXT search skipped (RAG_SKIP_TEXT=1)")
        res_t = {"ids":[[]], "documents":[[]], "metadatas":[[]], "distances":[[]]}
    else:
        t0 = perf_counter()
        qv_text = embed_e5_query(query)
        print("   [TEXT] querying ...")
        res_t = query_collection_safe(text_col, qvec=qv_text, topk=k, where=None, label="TEXT")
        ids_t = (res_t.get("ids") or [[]])[0]
        docs_t = (res_t.get("documents") or [[]])[0]
        print(f"   TEXT hits={len(ids_t)} ({perf_counter()-t0:.2f}s)")

    # 2) CODE(OpenAI) â€” ì–¸ì–´í•„í„° ìš°ì„ , 0ì´ë©´ í•„í„° í•´ì œ ì¬ì‹œë„
    if RAG_SKIP_CODE:
        print("   CODE search skipped (RAG_SKIP_CODE=1)")
        res_c = {"ids":[[]], "documents":[[]], "metadatas":[[]], "distances":[[]]}
    else:
        t1 = perf_counter()
        qv_code = embed_openai_query(query)
        where = {"code_lang": lang} if lang else None
        print(f"   [CODE] querying ... (filter={where})")
        res_c = query_collection_safe(code_col, qvec=qv_code, topk=k, where=where, label="CODE")
        ids_c = (res_c.get("ids") or [[]])[0]
        docs_c = (res_c.get("documents") or [[]])[0]
        if not ids_c and where is not None:
            print("   [CODE] 0 hits with filter â†’ retry without filter")
            res_c = query_collection_safe(code_col, qvec=qv_code, topk=k, where=None, label="CODE")
            ids_c = (res_c.get("ids") or [[]])[0]
            docs_c = (res_c.get("documents") or [[]])[0]
        print(f"   CODE hits={len(ids_c)} ({perf_counter()-t1:.2f}s)")

    # 3) RRF ìœµí•© (ê¸°ì¡´ ë¡œì§ ê·¸ëŒ€ë¡œ)
    ids_t = (res_t.get("ids") or [[]])[0]
    docs_t = (res_t.get("documents") or [[]])[0]
    ids_c = (res_c.get("ids") or [[]])[0]
    docs_c = (res_c.get("documents") or [[]])[0]

    fused_ids = rrf(ids_t, ids_c, k=60, w_a=0.6, w_b=0.4)
    pool = {i:d for i,d in zip(ids_t, docs_t)}
    for i,d in zip(ids_c, docs_c):
        pool.setdefault(i,d)
    docs_out = [pool[i] for i in fused_ids[:k] if i in pool]
    print(f"   FUSED topk={len(docs_out)}")
    return docs_out

# =========================
# ì§ˆì˜ ë¬¸ìì—´
# =========================
def build_query(issue: Dict[str, Any]) -> str:
    msg = (issue.get("message") or "").strip()
    tags = " ".join([str(t) for t in (issue.get("tags") or []) if t])
    rule = (issue.get("rule") or "").strip()
    base = " ".join([msg, tags, rule]).strip()
    # ë³´ì•ˆ ê°€ì´ë“œ íšŒìˆ˜ í–¥ìƒì„ ìœ„í•œ íŒíŠ¸(í•œêµ­ì–´ í‚¤ì›Œë“œ)
    return f"{base} ë³´ì•ˆ ê°€ì´ë“œ ê°œì„  ë°©ë²• ì·¨ì•½ì  ì›ì¸ í•´ê²°ì±… ëª¨ë²”ì‚¬ë¡€ ì½”ë“œì˜ˆì‹œ".strip()

# =========================
# LLM (CrewAI) - JSON ì‘ì„±
# =========================
def make_system_prompt() -> str:
    return (
        "ë‹¹ì‹ ì€ ì „ìì •ë¶€í”„ë ˆì„ì›Œí¬ ë³´ì•ˆê°œë°œê°€ì´ë“œì— ì •í†µí•œ ë³´ì•ˆ ê°ì‚¬ ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤. "
        "ì…ë ¥ ì´ìŠˆ(SonarQube)ì™€ ê²€ìƒ‰ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ, ë‹¤ìŒ JSON ìŠ¤í‚¤ë§ˆì— ì •í™•íˆ ë§ê²Œ ì‘ë‹µí•˜ì„¸ìš”. "
        "ì„¤ëª… ë¬¸ì¥ì´ë‚˜ ì½”ë“œë¸”ë¡ í‘œì‹œëŠ” ê¸ˆì§€í•©ë‹ˆë‹¤."
    )

def make_task_description(issue: Dict[str, Any], docs: List[str]) -> str:
    ctx = "\n\n".join([f"- {d}" for d in docs])
    return f"""
[ì´ìŠˆ]
- rule: {issue.get('rule')}
- severity: {issue.get('severity')}
- component: {issue.get('component')}
- line: {issue.get('line')}
- message: {issue.get('message')}
- tags: {', '.join(issue.get('tags') or [])}

[ê²€ìƒ‰ ë¬¸ë§¥(ìƒìœ„ {len(docs)}ê°œ)]
{ctx}

[JSON ìŠ¤í‚¤ë§ˆ]
{{
  "rule": "...",
  "severity": "...",
  "component": "...",
  "line": "...",
  "risk_level": "Critical|High|Medium|Low",
  "root_cause": "...",
  "impact": "...",
  "fix_guidance": [
    "êµ¬ì²´ì  ì¡°ì¹˜ 1",
    "êµ¬ì²´ì  ì¡°ì¹˜ 2"
  ],
  "secure_code_example": "ê°€ëŠ¥í•˜ë©´ ì§§ì€ ì½”ë“œ ìŠ¤ë‹ˆí«",
  "checklist": [
    "ì ê²€ í•­ëª© 1",
    "ì ê²€ í•­ëª© 2"
  ],
  "references": [
    "ê°€ì´ë“œ/ê·œì • í˜ì´ì§€ ë˜ëŠ” í‚¤ì›Œë“œ",
    "ê´€ë ¨ ê·œì¹™(rule) ì°¸ê³ "
  ]
}}
""".strip()

def parse_llm_json(txt: str, issue: Dict[str, Any]) -> Dict[str, Any]:
    s = str(txt).strip()
    if s.startswith("```"):
        s = s.strip("`").strip()
        if s.lower().startswith("json"):
            s = s[4:].strip()
    try:
        data = json.loads(s)
    except Exception:
        # ìµœì†Œ í´ë°±
        data = {
            "rule": issue.get("rule"),
            "severity": issue.get("severity"),
            "component": issue.get("component"),
            "line": issue.get("line"),
            "root_cause": issue.get("message"),
            "fix_guidance": [issue.get("message")],
            "checklist": [],
            "references": []
        }
    # ìŠ¤í‚¤ë§ˆ ëˆ„ë½ í•„ë“œ ë³´ì •
    data.setdefault("risk_level", "Low")
    data.setdefault("impact", "")
    data.setdefault("secure_code_example", "")
    data.setdefault("fix_guidance", [])
    data.setdefault("checklist", [])
    data.setdefault("references", [])
    return data

# =========================
# ë©”ì¸
# =========================
def main():
    # 0) ì…ë ¥
    issues = load_agent_inputs(AGENT_INPUTS_PATH)
    if LIMIT_ISSUES:
        issues = issues[:LIMIT_ISSUES]
    print(f"ğŸ—‚ issues: {len(issues)} | inputs: {AGENT_INPUTS_PATH.resolve()}")

    # 1) Chroma
    print(f"ğŸ”Œ opening Chroma at: {CHROMA_PERSIST_DIR}")
    client = init_chroma(CHROMA_PERSIST_DIR)
    print("ğŸ” listing collections...")
    names = [c.name for c in client.list_collections()]
    print("ğŸ“š collections found:", names)

    print(f"ğŸ“¦ getting collections: text='{TEXT_COLLECTION_NAME}', code='{CODE_COLLECTION_NAME}'")
    text_col, code_col = get_text_code_collections(client)
    print("âœ… collections fetched. (skip counting to avoid blocking)")

    # 2) LLM ì¤€ë¹„ (CrewAI)
    llm = LLM(
        model=OPENAI_CHAT_MODEL,
        temperature=0.2,
        api_key=os.getenv("OPENAI_API_KEY"),
    )
    security_agent = Agent(
        role="Security Auditor",
        goal="RAG ë¬¸ë§¥ì„ ê·¼ê±°ë¡œ Sonar ì´ìŠˆì˜ ë³´ì•ˆ ë¦¬ìŠ¤í¬ì™€ êµ¬ì²´ì  ê°œì„  ê°€ì´ë“œë¥¼ ì‚°ì¶œ",
        backstory="ì „ìì •ë¶€í”„ë ˆì„ì›Œí¬ ë³´ì•ˆê°œë°œ ê°€ì´ë“œì— ì •í†µ. ì‹¤ë¬´í˜• ì¡°ì–¸ ì œê³µì— íŠ¹í™”.",
        llm=llm,
        verbose=False,
        allow_delegation=False,
        max_iter=1,
        system_prompt=make_system_prompt(),
    )

    # 3) ì´ìŠˆë³„ ì²˜ë¦¬
    all_rows = []
    summary_rows = []
    for idx, issue in enumerate(issues, start=1):
        q = build_query(issue)
        lang = detect_lang(issue)
        print(f"[{idx}/{len(issues)}] {issue.get('component')}:{issue.get('line')} rule={issue.get('rule')} | lang={lang}")
        docs = retrieve_context_dual(text_col, code_col, q, lang=lang, k=5)

        task = Task(
            description=make_task_description(issue, docs),
            agent=security_agent,
            expected_output="ìœ„ ìŠ¤í‚¤ë§ˆì— ë§ëŠ” ë‹¨ì¼ JSON ë¬¸ìì—´"
        )
        crew = Crew(agents=[security_agent], tasks=[task], verbose=False)
        out = crew.kickoff()

        data = parse_llm_json(str(out), issue)
        data["_issue"] = issue  # ì›ë³¸ ë³´ì¡´

        # ì´ìŠˆë³„ JSON ì €ì¥
        fname = f"{sanitize_filename(issue.get('component') or 'issue')}__{issue.get('line','0')}.json"
        (OUTPUT_DIR / fname).write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")
        print(f"  â†’ saved {fname}")

        all_rows.append(data)
        summary_rows.append({
            "component": issue.get("component"),
            "line": issue.get("line"),
            "rule": issue.get("rule"),
            "output_file": fname
        })

    # 4) ì „ì²´ JSONL & summary.json
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    jsonl_path = OUTPUT_DIR / f"security_reports_{ts}.jsonl"
    with jsonl_path.open("w", encoding="utf-8") as f:
        for r in all_rows:
            f.write(json.dumps(r, ensure_ascii=False) + "\n")
    (OUTPUT_DIR / "summary.json").write_text(json.dumps(summary_rows, ensure_ascii=False, indent=2), encoding="utf-8")

    print(f"\nâœ… done: {len(all_rows)} items")
    print(f"   - {jsonl_path}")
    print(f"   - {OUTPUT_DIR / 'summary.json'}")

if __name__ == "__main__":
    main()
