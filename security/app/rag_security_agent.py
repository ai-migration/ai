# rag_security_agent.py
import os
import json
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any
from dotenv import load_dotenv
load_dotenv(dotenv_path=Path(__file__).with_name(".env"))

import chromadb
from chromadb.config import Settings
from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction

# --- Chroma & CrewAI / OpenAI ---
import chromadb
from chromadb.config import Settings
from crewai import Agent, Task, Crew, LLM
# CrewAI 0.51+ ê¸°ì¤€. í•˜ìœ„ ë²„ì „ì´ë©´ llm ì§€ì • ë°©ì‹/íŒŒë¼ë¯¸í„° ì¡°ê¸ˆ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ.
# ì„¤ì¹˜: pip install crewai chromadb openai tiktoken

import numpy as np
from sentence_transformers import SentenceTransformer
from openai import OpenAI
from time import perf_counter


# ====== ì„¤ì • ======
BASE = Path(__file__).resolve().parent
AGENT_INPUTS_PATH = BASE / "outputs" / "agent_inputs.json"
OUTPUT_DIR = BASE / "outputs" / "security_reports"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
LIMIT_ISSUES = 5

# ì—…ë¡œë“œí•œ chroma.sqlite3ê°€ í˜„ì¬ í´ë”ì— ìˆë‹¤ê³  ê°€ì • (ê²½ë¡œ ë°”ê¿”ë„ ë¨)
CHROMA_PERSIST_DIR = str(Path(".").resolve())           # sqlite íŒŒì¼ì´ ìˆëŠ” í´ë” ê²½ë¡œ
CHROMA_DB_FILE = Path("chroma.sqlite3")                 # sqlite íŒŒì¼ëª…
# ì»¬ë ‰ì…˜ ì´ë¦„ ëª¨ë¥´ë©´ ìë™ìœ¼ë¡œ ì²« ì»¬ë ‰ì…˜ ì‚¬ìš© ì‹œë„
DEFAULT_COLLECTION_NAME = None                          # ì˜ˆ: "security_guide" ë¡œ ì§€ì • ê°€ëŠ¥

OPENAI_MODEL = "gpt-4o-mini"  # ë¹„ìš©/ì†ë„ ê· í˜•ìš©. í•„ìš”ì‹œ ìƒìœ„ ëª¨ë¸ë¡œ.

# ====== í—¬í¼ ======

COLLECTION_KIND = {
    "security_code_oai3l": "code",  # OpenAI text-embedding-3-largeë¡œ ìƒ‰ì¸í•œ ì»¬ë ‰ì…˜
    "security_text_e5l": "text",     # E5(multilingual-e5-large)ë¡œ ìƒ‰ì¸í•œ ì»¬ë ‰ì…˜
}

class E5QueryEmbedder:
    def __init__(self, model_name="intfloat/multilingual-e5-large"):
        self.model = SentenceTransformer(model_name)
    def embed(self, query: str):
        vec = self.model.encode([f"query: {query}"], normalize_embeddings=True)
        return vec[0].tolist()

class OpenAIEmbedder:
    def __init__(self, model="text-embedding-3-large", api_key=None, dimensions=None):
        self.model = model
        self.dimensions = dimensions
        self.client = OpenAI(api_key=api_key) if api_key else OpenAI()
    def embed(self, text: str):
        resp = self.client.embeddings.create(model=self.model, input=[text], dimensions=self.dimensions)
        arr = np.array([e.embedding for e in resp.data], dtype=np.float32)
        nrm = np.linalg.norm(arr, axis=1, keepdims=True) + 1e-12
        return (arr / nrm)[0].tolist()

def load_agent_inputs(path: Path) -> List[Dict[str, Any]]:
    if not path.exists():
        raise FileNotFoundError(f"agent_inputs.jsonì´ ì—†ìŠµë‹ˆë‹¤: {path}")
    with path.open("r", encoding="utf-8") as f:
        return json.load(f)

def init_chroma_client(persist_dir: str) -> chromadb.ClientAPI:
    # sqlite íŒŒì¼ì´ ë™ì¼ í´ë”ì— ìˆê³ , PersistentClientê°€ í´ë” ë‹¨ìœ„ë¡œ ê´€ë¦¬í•¨
    client = chromadb.PersistentClient(
        path=persist_dir,
        settings=Settings(anonymized_telemetry=False)
    )
    return client


# ë“€ì–¼ RAG(ì½”ë“œ/í…ìŠ¤íŠ¸ ì»¬ë ‰ì…˜ ë™ì‹œ ê²€ìƒ‰) + ë³‘í•©
def get_collection(client, name: str | None):
    if name:
        return client.get_collection(name=name)  # â— embedding_function ì œê±°
    cols = client.list_collections()
    if not cols:
        raise RuntimeError("Chromaì— ì»¬ë ‰ì…˜ì´ ì—†ìŠµë‹ˆë‹¤.")
    return client.get_collection(name=cols[0].name)  # â— ë™ì¼

def build_query(issue: Dict[str, Any]) -> str:
    # message + tags + ruleì„ ë¬¶ì–´ ê²€ìƒ‰ ì§ˆì˜ ìƒì„±
    msg = (issue.get("message") or "").strip()
    tags = issue.get("tags") or []
    rule = (issue.get("rule") or "").strip()
    tag_str = " ".join([str(t) for t in tags if t])
    # ë³´ì•ˆ ê°€ì´ë“œ ê²€ìƒ‰ì— ë„ì›€ì´ ë˜ë„ë¡ í‚¤ì›Œë“œ ë³´ê°•
    base = f"{msg} {tag_str} {rule}".strip()
    return f"{base} ë³´ì•ˆ ê°€ì´ë“œ ê°œì„  ë°©ë²• ì·¨ì•½ì  ì›ì¸ í•´ê²°ì±… ëª¨ë²”ì‚¬ë¡€ ì½”ë“œì˜ˆì‹œ".strip()

def retrieve_context(collection, query: str, k: int = 5) -> List[str]:
    name = collection.name
    kind = COLLECTION_KIND.get(name, "text")  # ê¸°ë³¸ í…ìŠ¤íŠ¸ë¡œ

    try: 
        # [LOG] ì–´ë–¤ ì„ë² ë”©ìœ¼ë¡œ ì¿¼ë¦¬í•˜ëŠ”ì§€
        t0 = perf_counter()
        if kind == "code":
            print("ğŸ” query embedder: OpenAI text-embedding-3-large")
            emb = OpenAIEmbedder(model="text-embedding-3-large",
                                api_key=os.getenv("OPENAI_API_KEY")).embed(query)
            print(f"   âœ“ embedding ok ({perf_counter()-t0:.2f}s)")
        else:
            print("ğŸ” query embedder: E5 multilingual-e5-large (query: prefix)")
            emb = E5QueryEmbedder("intfloat/multilingual-e5-large").embed(query)
            print(f"   âœ“ embedding ok ({perf_counter()-t0:.2f}s)")
    except Exception as e:
        print(f"âŒ embedding error: {e}")
        return []

    try:
        print("   â†’ chroma.query ...")
        t1 = perf_counter()
        res = collection.query(
            query_embeddings=[emb],
            n_results=k,
            include=["documents","distances","ids","metadatas"]
        )
        docs = res.get("documents", [[]])[0]
        print(f"   â†³ retrieved docs: {len(docs)} ({perf_counter()-t1:.2f}s)")
        return docs
    except Exception as e:
        print(f"âŒ chroma query error: {e}")
        return []


def make_system_prompt() -> str:
    return (
        "ë‹¹ì‹ ì€ ì „ìì •ë¶€í”„ë ˆì„ì›Œí¬ ë³´ì•ˆê°œë°œê°€ì´ë“œì— ì •í†µí•œ ë³´ì•ˆ ê°ì‚¬ ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤. "
        "ì…ë ¥ ì´ìŠˆ(SonarQube)ì™€ RAGë¡œ ê²€ìƒ‰ëœ ê°€ì´ë“œë¥¼ ë°”íƒ•ìœ¼ë¡œ, ë³´ì•ˆ ìœ„í—˜ë„ í‰ê°€ì™€ ê°œì„  ê°€ì´ë“œë¥¼ "
        "ì •í™•í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”. ê°€ëŠ¥í•˜ë©´ ê°„ë‹¨í•œ ì½”ë“œ ìˆ˜ì • ì˜ˆë„ í¬í•¨í•˜ì„¸ìš”. "
        "ì¶œë ¥ì€ ë°˜ë“œì‹œ JSON ìŠ¤í‚¤ë§ˆë¥¼ ì¤€ìˆ˜í•˜ì„¸ìš”."
    )

def make_task_description(issue: Dict[str, Any], docs: List[str]) -> str:
    # ì´ìŠˆ ë° ê²€ìƒ‰ ë¬¸ë§¥ì„ íƒœìŠ¤í¬ì— ì£¼ì…
    ctx = "\n\n".join([f"- {d}" for d in docs])
    return f"""
[ì´ìŠˆ]
- rule: {issue.get('rule')}
- severity: {issue.get('severity')}
- component: {issue.get('component')}
- line: {issue.get('line')}
- message: {issue.get('message')}
- tags: {', '.join(issue.get('tags') or [])}

[ê²€ìƒ‰ ë¬¸ë§¥(ë³´ì•ˆ ê°€ì´ë“œ ë°œì·Œ, ìƒìœ„ {len(docs)}ê°œ)]
{ctx}

[ìš”êµ¬ì‚¬í•­]
ì•„ë˜ JSON ìŠ¤í‚¤ë§ˆë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”. ì„¤ëª… ë¬¸ì¥ ì¶”ê°€ ê¸ˆì§€.

{{
  "rule": "...",
  "severity": "...",
  "component": "...",
  "line": "...",
  "risk_level": "Critical|High|Medium|Low",
  "root_cause": "...",
  "impact": "...",
  "fix_guidance": [
    "êµ¬ì²´ì  ì¡°ì¹˜ 1",
    "êµ¬ì²´ì  ì¡°ì¹˜ 2",
    "..."
  ],
  "secure_code_example": "ê°€ëŠ¥í•˜ë©´ ì§§ì€ ì½”ë“œ ìŠ¤ë‹ˆí«",
  "checklist": [
    "ì ê²€ í•­ëª© 1",
    "ì ê²€ í•­ëª© 2"
  ],
  "references": [
    "ê°€ì´ë“œ/ê·œì • í˜ì´ì§€ ë˜ëŠ” í‚¤ì›Œë“œ",
    "ê´€ë ¨ ê·œì¹™(rule) ì°¸ê³ "
  ]
}}
""".strip()

def save_jsonl(rows: List[Dict[str, Any]], path: Path):
    with path.open("w", encoding="utf-8") as f:
        for r in rows:
            f.write(json.dumps(r, ensure_ascii=False) + "\n")

def write_markdown(issue: Dict[str, Any], result: Dict[str, Any], out_dir: Path):
    fn = f"{issue.get('component','unknown').replace('/','_')}_{issue.get('line','-')}.md"
    p = out_dir / fn
    md = [
        f"# ë³´ì•ˆ ê°œì„  ê°€ì´ë“œ: {issue.get('component')}:{issue.get('line')}",
        f"- Rule: `{result.get('rule','')}`",
        f"- Severity: `{result.get('severity','')}`",
        f"- Risk: `{result.get('risk_level','')}`",
        "",
        "## ì›ì¸(Root Cause)",
        result.get("root_cause",""),
        "",
        "## ì˜í–¥(Impact)",
        result.get("impact",""),
        "",
        "## ê°œì„  ê°€ì´ë“œ",
        *[f"- {s}" for s in result.get("fix_guidance",[])],
        "",
        "## ì•ˆì „í•œ ì½”ë“œ ì˜ˆì‹œ",
        "```",
        (result.get("secure_code_example") or "").strip(),
        "```",
        "",
        "## ì ê²€ ì²´í¬ë¦¬ìŠ¤íŠ¸",
        *[f"- [ ] {c}" for c in result.get("checklist",[])],
        "",
        "## ì°¸ê³ ",
        *[f"- {r}" for r in result.get("references",[])]
    ]
    p.write_text("\n".join(md), encoding="utf-8")
    return p

def main():
    # 1) ì…ë ¥ ë¡œë“œ
    issues = load_agent_inputs(AGENT_INPUTS_PATH)
    # [LOG] agent_inputs ìƒíƒœ í™•ì¸
    print(f"ğŸ—‚ loaded: {len(issues)} issues from {AGENT_INPUTS_PATH.resolve()}")
    if not issues:
        print("âŒ agent_inputsê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. run_sonar.py â†’ extract_agent_inputs.py ìˆœì„œ/ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return

    # 2) Chroma ì ‘ì† & ì»¬ë ‰ì…˜ ì„ íƒ
    if not CHROMA_DB_FILE.exists():
        print(f"âš ï¸ ê²½ê³ : {CHROMA_DB_FILE} ê°€ í˜„ì¬ í´ë”ì— ì—†ìŠµë‹ˆë‹¤. CHROMA_PERSIST_DIR ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    client = init_chroma_client(CHROMA_PERSIST_DIR)
    collection = get_collection(client, DEFAULT_COLLECTION_NAME)
    kind = COLLECTION_KIND.get(collection.name, "text")
    print(f"âœ… RAG ì»¬ë ‰ì…˜ ì‚¬ìš©: {collection.name} (kind={kind})")

    # 3) CrewAI ì—ì´ì „íŠ¸
    chat_model = os.getenv("OPENAI_CHAT_MODEL", OPENAI_MODEL)  # ê¸°ë³¸ gpt-4o-mini
    llm = LLM(
        model=chat_model,                    # ì˜ˆ: "gpt-4o-mini"  (í•„ìš”ì‹œ "openai/gpt-4o-mini"ë¡œ)
        temperature=0.2,
        api_key=os.getenv("OPENAI_API_KEY"), # .env ë¡œë“œë¨
        # base_url=os.getenv("OPENAI_BASE_URL")  # í”„ë¡ì‹œ/ê²Œì´íŠ¸ì›¨ì´ ì“°ë©´ ì„¤ì •
    )

    security_agent = Agent(
        role="Security Auditor",
        goal="RAG ë¬¸ë§¥ì„ ê·¼ê±°ë¡œ Sonar ì´ìŠˆì˜ ë³´ì•ˆ ë¦¬ìŠ¤í¬ì™€ êµ¬ì²´ì  ê°œì„  ê°€ì´ë“œë¥¼ ì‚°ì¶œ",
        backstory="ì „ìì •ë¶€í”„ë ˆì„ì›Œí¬ ë³´ì•ˆê°œë°œ ê°€ì´ë“œì— ì •í†µ. ì‹¤ë¬´í˜• ì¡°ì–¸ ì œê³µì— íŠ¹í™”.",
        llm=llm,
        verbose=False,
        allow_delegation=False,
        max_iter=1
    )



    results_jsonl = []
    issues = issues[:LIMIT_ISSUES]
    for idx, issue in enumerate(issues, start=1):
        query = build_query(issue)
        docs = retrieve_context(collection, query, k=5)

        task = Task(
            description=make_task_description(issue, docs),
            agent=security_agent,
            expected_output="ìœ„ JSON ìŠ¤í‚¤ë§ˆì— ë§ëŠ” ë‹¨ì¼ JSON ë¬¸ìì—´"
        )

        crew = Crew(agents=[security_agent], tasks=[task], verbose=False)
        print(f"[{idx}/{len(issues)}] ë¶„ì„ ì¤‘: {issue.get('component')}:{issue.get('line')} ({issue.get('rule')})")
        out = crew.kickoff()  # ë¬¸ìì—´(JSON í˜•ì‹ ê¸°ëŒ€)

        # ëª¨ë¸ ì¶œë ¥ JSON íŒŒì‹± ë³´ì •
        txt = str(out).strip()
        # ì½”ë“œë¸”ë¡ë¡œ ê°ì‹¸ëŠ” ëª¨ë¸ ìŠµê´€ ë°©ì§€
        if txt.startswith("```"):
            txt = txt.strip("`").strip()
            # ì–¸ì–´íƒœê·¸ ì œê±° ê°€ëŠ¥
            if txt.lower().startswith("json"):
                txt = txt[4:].strip()

        try:
            data = json.loads(txt)
        except Exception:
            # í˜¹ì‹œ JSONì´ ì•„ë‹ˆë©´ ìµœì†Œ ë˜í•‘
            data = {
                "rule": issue.get("rule"),
                "severity": issue.get("severity"),
                "component": issue.get("component"),
                "line": issue.get("line"),
                "raw_output": txt
            }

        # ì›ë³¸ ì´ìŠˆ í•„ë“œ ë¶€ê°€ ì €ì¥
        data["_issue"] = issue
        results_jsonl.append(data)

        # Markdown ë³´ê³ ì„œë„ íŒŒì¼ë¡œ ì €ì¥
        md_path = write_markdown(issue, data, OUTPUT_DIR)
        print(f"  â†’ ë³´ê³ ì„œ ì €ì¥: {md_path.name}")

    # 4) ì „ì²´ ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    out_jsonl = OUTPUT_DIR / f"security_reports_{timestamp}.jsonl"
    save_jsonl(results_jsonl, out_jsonl)
    print(f"\nâœ… ì™„ë£Œ: {out_jsonl} (ì´ {len(results_jsonl)}ê±´)")

if __name__ == "__main__":
    main()
